{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Programm to Test The Pooling Custom Layer TensorRT Plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 225\n",
    "win_size = 3\n",
    "stride = 2\n",
    "output_size = input_size // stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "x = np.random.randn(3, input_size, input_size)\n",
    "x = np.float32(x)\n",
    "input_tensor = torch.tensor(x)\n",
    "input_batch = input_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 225, 225])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.5237e-01,  1.2867e-03,  3.4907e-01,  ..., -4.2639e-01,\n",
       "           -2.5910e+00,  6.5477e-01],\n",
       "          [ 1.2469e+00,  9.9521e-01, -9.3283e-02,  ..., -7.1065e-02,\n",
       "           -7.8114e-01,  1.3318e+00],\n",
       "          [ 7.7094e-01,  7.2175e-01,  4.2063e-02,  ...,  6.2130e-01,\n",
       "            2.0740e-01, -4.7347e-01],\n",
       "          ...,\n",
       "          [-1.4965e+00,  4.2837e-01,  8.5091e-01,  ..., -8.7340e-01,\n",
       "           -8.1079e-01, -1.1424e+00],\n",
       "          [ 4.1352e-01, -5.0900e-01, -1.5102e+00,  ..., -5.0868e-01,\n",
       "           -1.4747e+00, -1.0031e+00],\n",
       "          [-1.2912e+00,  9.9546e-01, -8.2836e-01,  ..., -2.7126e-01,\n",
       "            9.4103e-02, -1.6816e+00]],\n",
       "\n",
       "         [[-2.6476e+00,  8.3387e-01,  2.4967e-01,  ...,  4.2392e-01,\n",
       "           -1.4385e+00,  1.9493e-01],\n",
       "          [ 1.6934e+00, -2.3278e+00, -7.3203e-01,  ...,  6.7910e-01,\n",
       "            2.7497e-01,  1.4299e+00],\n",
       "          [ 4.8838e-02, -1.2762e+00, -8.2371e-01,  ...,  3.2309e-01,\n",
       "            1.1854e-01,  8.3346e-01],\n",
       "          ...,\n",
       "          [-2.1186e-02,  1.2026e+00,  1.7494e+00,  ...,  6.5488e-01,\n",
       "            1.0239e-01, -2.2898e+00],\n",
       "          [-1.6344e+00,  3.4890e-01,  1.4792e+00,  ..., -8.9314e-03,\n",
       "           -1.2492e+00, -5.8376e-01],\n",
       "          [-1.0079e-01,  1.9098e+00, -6.3659e-01,  ..., -9.1867e-01,\n",
       "            2.1217e-03, -9.2152e-01]],\n",
       "\n",
       "         [[ 1.3963e+00,  1.7590e-01,  9.5128e-01,  ...,  1.9335e+00,\n",
       "           -1.1361e+00,  1.2501e+00],\n",
       "          [-6.4332e-01,  9.0983e-01,  3.7324e-01,  ..., -2.2008e-01,\n",
       "            1.1038e+00,  1.5373e-01],\n",
       "          [ 6.2944e-01, -4.1922e-01,  1.7477e+00,  ...,  9.3438e-01,\n",
       "            9.1986e-01, -1.3666e+00],\n",
       "          ...,\n",
       "          [ 8.7998e-01,  7.3590e-01, -1.9414e-01,  ..., -6.8106e-01,\n",
       "           -7.8261e-02,  3.1241e-01],\n",
       "          [ 1.9878e+00, -3.3150e-01, -4.3106e-01,  ..., -7.8804e-02,\n",
       "            8.9352e-01,  9.6161e-01],\n",
       "          [ 1.1549e+00,  1.0303e+00,  1.4278e+00,  ...,  4.4380e-01,\n",
       "            1.0537e+00, -7.4268e-01]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_trt_network(network):\n",
    "    # Input\n",
    "    input_tensor = network.add_input(name='input', dtype=trt.float32, shape=(1, 3, input_size, input_size))\n",
    "\n",
    "    # MaxPool2d\n",
    "    layer = network.add_pooling_nd( \\\n",
    "        input=input_tensor, type=trt.PoolingType.MAX, window_size=(win_size, win_size))\n",
    "    layer.stride_nd = (stride, stride)\n",
    "\n",
    "    # Output\n",
    "    layer.get_output(0).name = 'output'\n",
    "    network.mark_output(tensor=layer.get_output(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trt_analyzer\n",
    "import tensorrt as trt\n",
    "\n",
    "EXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "\n",
    "def build_engine(logger):\n",
    "    with trt.Builder(logger) as builder, builder.create_network(EXPLICIT_BATCH) as network, builder.create_builder_config() as config, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "        if hasattr(config, 'set_memory_pool_limit'):\n",
    "            config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)\n",
    "        else:\n",
    "            config.max_workspace_size = 1 << 30\n",
    "        # Define the TRT network using weights from the PyTorch model.\n",
    "        #define_trt_plugin_network(network)\n",
    "        define_trt_network(network)\n",
    "        # Get network info\n",
    "        global net_dict\n",
    "        net_dict = trt_analyzer.network_dict(network)\n",
    "        # Build and return an engine.\n",
    "        plan = builder.build_serialized_network(network, config)\n",
    "        engine = runtime.deserialize_cuda_engine(plan)\n",
    "        return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/27/2022-08:30:49] [TRT] [I] [MemUsageChange] Init CUDA: CPU +201, GPU +0, now: CPU 284, GPU 2776 (MiB)\n",
      "[06/27/2022-08:30:53] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +140, GPU +132, now: CPU 444, GPU 2923 (MiB)\n",
      "[06/27/2022-08:30:53] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 444, GPU 2923 (MiB)\n",
      "[06/27/2022-08:30:53] [TRT] [V] Applying generic optimizations to the graph for inference.\n",
      "[06/27/2022-08:30:53] [TRT] [V] Original: 1 layers\n",
      "[06/27/2022-08:30:53] [TRT] [V] After dead-layer removal: 1 layers\n",
      "[06/27/2022-08:30:53] [TRT] [V] After Myelin optimization: 1 layers\n",
      "[06/27/2022-08:30:53] [TRT] [V] Applying ScaleNodes fusions.\n",
      "[06/27/2022-08:30:53] [TRT] [V] After scale fusion: 1 layers\n",
      "[06/27/2022-08:30:53] [TRT] [V] After vertical fusions: 1 layers\n",
      "[06/27/2022-08:30:53] [TRT] [V] After dupe layer removal: 1 layers\n",
      "[06/27/2022-08:30:53] [TRT] [V] After final dead-layer removal: 1 layers\n",
      "[06/27/2022-08:30:53] [TRT] [V] After tensor merging: 1 layers\n",
      "[06/27/2022-08:30:53] [TRT] [V] After slice removal: 1 layers\n",
      "[06/27/2022-08:30:53] [TRT] [V] After concat removal: 1 layers\n",
      "[06/27/2022-08:30:53] [TRT] [V] Graph construction and optimization completed in 0.00219304 seconds.\n",
      "[06/27/2022-08:30:53] [TRT] [I] ---------- Layers Running on DLA ----------\n",
      "[06/27/2022-08:30:53] [TRT] [I] ---------- Layers Running on GPU ----------\n",
      "[06/27/2022-08:30:53] [TRT] [I] [GpuLayer] POOLING: (Unnamed Layer* 0) [Pooling]\n",
      "[06/27/2022-08:30:55] [TRT] [V] Using cublasLt as a tactic source\n",
      "[06/27/2022-08:30:55] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +260, GPU +247, now: CPU 704, GPU 3170 (MiB)\n",
      "[06/27/2022-08:30:55] [TRT] [V] Using cuDNN as a tactic source\n",
      "[06/27/2022-08:30:55] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +84, GPU +79, now: CPU 788, GPU 3249 (MiB)\n",
      "[06/27/2022-08:30:55] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[06/27/2022-08:30:55] [TRT] [V] Constructing optimization profile number 0 [1/1].\n",
      "[06/27/2022-08:30:55] [TRT] [V] Reserving memory for activation tensors. Host: 0 bytes Device: 758028 bytes\n",
      "[06/27/2022-08:30:55] [TRT] [V] =============== Computing reformatting costs\n",
      "[06/27/2022-08:30:55] [TRT] [V] =============== Computing reformatting costs\n",
      "[06/27/2022-08:30:55] [TRT] [V] =============== Computing costs for \n",
      "[06/27/2022-08:30:55] [TRT] [V] *************** Autotuning format combination: Float(151875,50625,225,1) -> Float(37632,12544,112,1) ***************\n",
      "[06/27/2022-08:30:55] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 0) [Pooling] (TiledPooling)\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 257 Time: 0.208992\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 65793 Time: 0.191851\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 131329 Time: 0.249141\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 196865 Time: 0.265616\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 262401 Time: 0.230448\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 327937 Time: 0.241232\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 393473 Time: 0.242725\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 459009 Time: 0.175525\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 524545 Time: 0.159947\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 590081 Time: 0.199077\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 655617 Time: 0.207499\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 721153 Time: 0.182539\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 786689 Time: 0.186096\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 852225 Time: 0.192821\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 917761 Time: 0.151986\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 983297 Time: 0.144188\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 1048833 Time: 0.162119\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 1114369 Time: 0.154496\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 1179905 Time: 0.140338\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 1245441 Time: 0.139292\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 1310977 Time: 0.150763\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 1376513 Time: 0.152149\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 1442049 Time: 0.144245\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 1507585 Time: 0.164452\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 1573121 Time: 0.161308\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 1638657 Time: 0.148974\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 1704193 Time: 0.145486\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 1769729 Time: 0.155268\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 1835265 Time: 0.155499\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 1900801 Time: 0.143698\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 1966337 Time: 0.168725\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 2031873 Time: 0.166692\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 2097409 Time: 0.155278\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 2162945 Time: 0.147719\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 2228481 Time: 0.164046\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 2294017 Time: 0.0922027\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 2359553 Time: 0.0830699\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 2425089 Time: 0.098424\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 2490625 Time: 0.0982373\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 2556161 Time: 0.090328\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 2621697 Time: 0.0913413\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 2687233 Time: 0.093896\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: 6947073 Time: 0.0786795\n",
      "[06/27/2022-08:30:55] [TRT] [V] Fastest Tactic: 6947073 Time: 0.0786795\n",
      "[06/27/2022-08:30:55] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 0) [Pooling] (CudnnPooling)\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: -1 Time: 0.110749\n",
      "[06/27/2022-08:30:55] [TRT] [V] Fastest Tactic: -1 Time: 0.110749\n",
      "[06/27/2022-08:30:55] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 0) [Pooling] (CaskPooling)\n",
      "[06/27/2022-08:30:55] [TRT] [V] (Unnamed Layer* 0) [Pooling] Set Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NCHW_Max Tactic: -5359392427276731246\n",
      "[06/27/2022-08:30:55] [TRT] [V] Tactic: -5359392427276731246 Time: 0.115278\n",
      "[06/27/2022-08:30:55] [TRT] [V] Fastest Tactic: -5359392427276731246 Time: 0.115278\n",
      "[06/27/2022-08:30:55] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: TiledPooling Tactic: 6947073\n",
      "[06/27/2022-08:30:55] [TRT] [V] Formats and tactics selection completed in 0.110138 seconds.\n",
      "[06/27/2022-08:30:55] [TRT] [V] After reformat layers: 1 layers\n",
      "[06/27/2022-08:30:55] [TRT] [V] Pre-optimized block assignment.\n",
      "[06/27/2022-08:30:55] [TRT] [V] Block size 1073741824\n",
      "[06/27/2022-08:30:55] [TRT] [V] Total Activation Memory: 1073741824\n",
      "[06/27/2022-08:30:55] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[06/27/2022-08:30:55] [TRT] [V] Layer: (Unnamed Layer* 0) [Pooling] Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
      "[06/27/2022-08:30:55] [TRT] [I] Total Host Persistent Memory: 0\n",
      "[06/27/2022-08:30:55] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[06/27/2022-08:30:55] [TRT] [I] Total Scratch Memory: 0\n",
      "[06/27/2022-08:30:55] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 1 MiB\n",
      "[06/27/2022-08:30:55] [TRT] [V] Optimized block assignment.\n",
      "[06/27/2022-08:30:55] [TRT] [I] Total Activation Memory: 0\n",
      "[06/27/2022-08:30:55] [TRT] [V] Disabling unused tactic source: CUDNN\n",
      "[06/27/2022-08:30:55] [TRT] [V] Disabling unused tactic source: CUBLAS, CUBLASLT\n",
      "[06/27/2022-08:30:55] [TRT] [V] Engine generation completed in 2.18594 seconds.\n",
      "[06/27/2022-08:30:55] [TRT] [V] Deleting timing cache: 1 entries, served 0 hits since creation.\n",
      "[06/27/2022-08:30:55] [TRT] [V] Engine Layer Information:\n",
      "Layer(TiledPooling): (Unnamed Layer* 0) [Pooling], Tactic: 6947073, input[Float(1,3,225,225)] -> output[Float(1,3,112,112)]\n",
      "[06/27/2022-08:30:55] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)\n",
      "[06/27/2022-08:30:55] [TRT] [I] Loaded engine size: 0 MiB\n",
      "[06/27/2022-08:30:55] [TRT] [V] Deserialization required 655 microseconds.\n",
      "[06/27/2022-08:30:55] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)\n",
      "[06/27/2022-08:30:55] [TRT] [V] Total per-runner device persistent memory is 0\n",
      "[06/27/2022-08:30:55] [TRT] [V] Total per-runner host persistent memory is 0\n",
      "[06/27/2022-08:30:55] [TRT] [V] Allocated activation device memory of size 0\n",
      "[06/27/2022-08:30:55] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import common\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.VERBOSE)\n",
    "net_dict = None\n",
    "with build_engine(TRT_LOGGER) as engine:\n",
    "    inputs, outputs, bindings, stream = common.allocate_buffers(engine)\n",
    "    with engine.create_execution_context() as context:\n",
    "        inputs[0].host = input_batch.numpy()\n",
    "        trt_outputs = common.do_inference_v2( \\\n",
    "            context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Inputs</th>\n",
       "      <th>Outputs</th>\n",
       "      <th>Type Specific Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Unnamed Layer* 0) [Pooling]</td>\n",
       "      <td>LayerType.POOLING</td>\n",
       "      <td>(1, 3, 225, 225)</td>\n",
       "      <td>(1, 3, 112, 112)</td>\n",
       "      <td>type=PoolingType.MAX wsize=(3, 3) stride=(2, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Name               Type             Inputs  \\\n",
       "0  (Unnamed Layer* 0) [Pooling]  LayerType.POOLING   (1, 3, 225, 225)   \n",
       "\n",
       "             Outputs                               Type Specific Params  \n",
       "0   (1, 3, 112, 112)  type=PoolingType.MAX wsize=(3, 3) stride=(2, 2...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(net_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.2468637  0.9731418  1.7283603  ... 1.7039526  1.7803113  1.3318315 ]\n",
      "  [0.87263244 1.6231679  1.7283603  ... 1.0964074  1.6265202  2.3499799 ]\n",
      "  [0.87263244 0.8002932  0.8002932  ... 1.4108361  1.6265202  2.3499799 ]\n",
      "  ...\n",
      "  [1.8190268  2.0853243  2.0853243  ... 2.7980096  2.7980096  1.5694978 ]\n",
      "  [0.85090864 2.0072181  2.0072181  ... 2.075235   1.7450305  0.03003781]\n",
      "  [0.99545527 2.0072181  2.0072181  ... 2.075235   1.7450305  0.09410294]]\n",
      "\n",
      " [[1.6933644  1.4559153  0.5049344  ... 1.1002792  1.566453   1.4299002 ]\n",
      "  [2.2559497  2.2559497  0.36667484 ... 1.1002792  1.566453   0.83345956]\n",
      "  [2.2559497  2.2559497  1.0379118  ... 2.6732693  1.2995731  1.2995731 ]\n",
      "  ...\n",
      "  [1.3822354  1.1411262  1.017575   ... 1.4276408  1.6172732  1.8445979 ]\n",
      "  [1.7493899  1.7493899  1.4588192  ... 1.4615191  1.6172732  1.0922401 ]\n",
      "  [1.9097644  1.7493899  1.6592412  ... 1.4615191  1.2949623  0.6548751 ]]\n",
      "\n",
      " [[1.7477155  1.7477155  1.4374763  ... 1.4319489  1.9334582  1.9334582 ]\n",
      "  [1.7477155  1.7477155  1.4770554  ... 1.823906   1.823906   0.9343786 ]\n",
      "  [1.4112515  2.1228704  2.1228704  ... 1.9349664  2.7011197  1.1517128 ]\n",
      "  ...\n",
      "  [2.4014568  2.9250915  2.9250915  ... 0.70698655 0.5312173  1.7811435 ]\n",
      "  [0.8799799  2.9250915  2.9250915  ... 0.92300457 2.0150304  2.0150304 ]\n",
      "  [1.9877678  1.4277685  1.18733    ... 1.6709127  2.0669277  1.0536616 ]]]\n"
     ]
    }
   ],
   "source": [
    "reference = trt_outputs[0].reshape((3, input_size // 2, input_size // 2))\n",
    "print(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "cur_path = %pwd\n",
    "plugin_path = os.path.join(cur_path, 'plugin')\n",
    "sys.path.append(plugin_path)\n",
    "from trt_plugin_pb2 import copy_Message\n",
    "from trt_plugin_pb2 import pooling_Message\n",
    "import trt_plugin_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "\n",
    "lib_file = os.path.join(plugin_path, 'build', 'libPoolingPlugin.so')\n",
    "lib = ctypes.CDLL(lib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CustomQKVToContextPluginDynamic', 'CustomQKVToContextPluginDynamic', 'CustomQKVToContextPluginDynamic', 'RnRes2Br1Br2c_TRT', 'RnRes2Br1Br2c_TRT', 'RnRes2FullFusion_TRT', 'SmallTileGEMM_TRT', 'RNNTEncoderPlugin', 'DLRM_BOTTOM_MLP_TRT', 'CustomSkipLayerNormPluginDynamic', 'CustomSkipLayerNormPluginDynamic', 'CustomSkipLayerNormPluginDynamic', 'CustomSkipLayerNormPluginDynamic', 'SingleStepLSTMPlugin', 'RnRes2Br2bBr2c_TRT', 'RnRes2Br2bBr2c_TRT', 'CustomGeluPluginDynamic', 'CustomFCPluginDynamic', 'CustomEmbLayerNormPluginDynamic', 'CustomEmbLayerNormPluginDynamic', 'CustomEmbLayerNormPluginDynamic', 'pooling', 'copy']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'macnica_trt_plugins', 'macnica_trt_plugins']\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "\n",
    "registry = trt.get_plugin_registry()\n",
    "print([c.name for c in registry.plugin_creator_list])\n",
    "print([c.plugin_namespace for c in registry.plugin_creator_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = 'macnica_trt_plugins'\n",
    "macnica_creators = [c for c in registry.plugin_creator_list if c.plugin_namespace == namespace]\n",
    "for c in macnica_creators:\n",
    "    registry.register_creator(c, namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_trt_plugin_network(network):\n",
    "    # Input\n",
    "    input_tensor = network.add_input(name='input', dtype=trt.float32, shape=(1, 3, input_size, input_size))\n",
    "    \n",
    "    ### Custom Pooling Layer with CUDA or cuDNN ###\n",
    "    creator = registry.get_plugin_creator( \\\n",
    "        type='pooling', version='1', plugin_namespace='macnica_trt_plugins')\n",
    "    sz = input_tensor.shape\n",
    "    message = pooling_Message( \\\n",
    "        dims=sz, mode=trt_plugin_pb2.Maximum, window=[win_size, win_size], \\\n",
    "        stride=[stride, stride], impl=trt_plugin_pb2.CUDA)\n",
    "    plg = creator.deserialize_plugin('pooling', message.SerializeToString())\n",
    "    layer = network.add_plugin_v2(inputs=[input_tensor], plugin=plg)\n",
    "\n",
    "    # Output\n",
    "    layer.get_output(0).name = 'output'\n",
    "    network.mark_output(tensor=layer.get_output(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trt_analyzer\n",
    "import tensorrt as trt\n",
    "\n",
    "EXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "\n",
    "def build_engine2(logger):\n",
    "    with trt.Builder(logger) as builder, builder.create_network(EXPLICIT_BATCH) as network, builder.create_builder_config() as config, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "        if hasattr(config, 'set_memory_pool_limit'):\n",
    "            config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)\n",
    "        else:\n",
    "            config.max_workspace_size = 1 << 30\n",
    "        # Define the TRT network using weights from the PyTorch model.\n",
    "        define_trt_plugin_network(network)\n",
    "        #define_trt_network(network)\n",
    "        # Get network info\n",
    "        global net_dict\n",
    "        net_dict = trt_analyzer.network_dict(network)\n",
    "        # Build and return an engine.\n",
    "        plan = builder.build_serialized_network(network, config)\n",
    "        engine = runtime.deserialize_cuda_engine(plan)\n",
    "        return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/27/2022-08:30:56] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 793, GPU 3264 (MiB)\n",
      "[06/27/2022-08:30:56] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 793, GPU 3264 (MiB)\n",
      "[06/27/2022-08:30:56] [TRT] [V] Applying generic optimizations to the graph for inference.\n",
      "[06/27/2022-08:30:56] [TRT] [V] Original: 1 layers\n",
      "[06/27/2022-08:30:56] [TRT] [V] After dead-layer removal: 1 layers\n",
      "[06/27/2022-08:30:56] [TRT] [V] After Myelin optimization: 1 layers\n",
      "[06/27/2022-08:30:56] [TRT] [V] Applying ScaleNodes fusions.\n",
      "[06/27/2022-08:30:56] [TRT] [V] After scale fusion: 1 layers\n",
      "[06/27/2022-08:30:56] [TRT] [V] After vertical fusions: 1 layers\n",
      "[06/27/2022-08:30:56] [TRT] [V] After dupe layer removal: 1 layers\n",
      "[06/27/2022-08:30:56] [TRT] [V] After final dead-layer removal: 1 layers\n",
      "[06/27/2022-08:30:56] [TRT] [V] After tensor merging: 1 layers\n",
      "[06/27/2022-08:30:56] [TRT] [V] After slice removal: 1 layers\n",
      "[06/27/2022-08:30:56] [TRT] [V] After concat removal: 1 layers\n",
      "[06/27/2022-08:30:56] [TRT] [V] Graph construction and optimization completed in 0.00177704 seconds.\n",
      "[06/27/2022-08:30:56] [TRT] [I] ---------- Layers Running on DLA ----------\n",
      "[06/27/2022-08:30:56] [TRT] [I] ---------- Layers Running on GPU ----------\n",
      "[06/27/2022-08:30:56] [TRT] [I] [GpuLayer] PLUGIN_V2: (Unnamed Layer* 0) [PluginV2Ext]\n",
      "[06/27/2022-08:30:56] [TRT] [V] Using cublasLt as a tactic source\n",
      "[06/27/2022-08:30:56] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 793, GPU 3264 (MiB)\n",
      "[06/27/2022-08:30:56] [TRT] [V] Using cuDNN as a tactic source\n",
      "[06/27/2022-08:30:56] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 793, GPU 3264 (MiB)\n",
      "[06/27/2022-08:30:56] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[06/27/2022-08:30:56] [TRT] [V] Constructing optimization profile number 0 [1/1].\n",
      "[06/27/2022-08:30:56] [TRT] [V] Reserving memory for activation tensors. Host: 0 bytes Device: 758028 bytes\n",
      "[06/27/2022-08:30:56] [TRT] [V] =============== Computing reformatting costs\n",
      "[06/27/2022-08:30:56] [TRT] [V] =============== Computing reformatting costs\n",
      "[06/27/2022-08:30:56] [TRT] [V] =============== Computing costs for \n",
      "[06/27/2022-08:30:56] [TRT] [V] *************** Autotuning format combination: Float(151875,50625,225,1) -> Float(37632,12544,112,1) ***************\n",
      "[06/27/2022-08:30:56] [TRT] [V] --------------- Timing Runner: (Unnamed Layer* 0) [PluginV2Ext] (PluginV2)\n",
      "Process started.\n",
      "Process finished.\n",
      "Process started.\n",
      "Process finished.\n",
      "Process started.\n",
      "Process finished.\n",
      "Process started.\n",
      "Process finished.\n",
      "Process started.\n",
      "Process finished.\n",
      "Process started.\n",
      "Process finished.\n",
      "Process started.\n",
      "Process finished.\n",
      "Process started.\n",
      "Process finished.\n",
      "Process started.\n",
      "Process finished.\n",
      "Process started.\n",
      "Process finished.\n",
      "Process started.\n",
      "Process finished.\n",
      "Process started.\n",
      "Process finished.\n",
      "[06/27/2022-08:30:56] [TRT] [V] Tactic: 0 Time: 0.132455\n",
      "[06/27/2022-08:30:56] [TRT] [V] Fastest Tactic: 0 Time: 0.132455\n",
      "[06/27/2022-08:30:56] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PluginV2 Tactic: 0\n",
      "[06/27/2022-08:30:56] [TRT] [V] Formats and tactics selection completed in 0.264201 seconds.\n",
      "[06/27/2022-08:30:56] [TRT] [V] After reformat layers: 1 layers\n",
      "[06/27/2022-08:30:56] [TRT] [V] Pre-optimized block assignment.\n",
      "[06/27/2022-08:30:56] [TRT] [V] Block size 1073741824\n",
      "[06/27/2022-08:30:56] [TRT] [V] Total Activation Memory: 1073741824\n",
      "[06/27/2022-08:30:56] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[06/27/2022-08:30:56] [TRT] [V] Layer: (Unnamed Layer* 0) [PluginV2Ext] Host Persistent: 16 Device Persistent: 0 Scratch Memory: 0\n",
      "[06/27/2022-08:30:56] [TRT] [I] Total Host Persistent Memory: 16\n",
      "[06/27/2022-08:30:56] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[06/27/2022-08:30:56] [TRT] [I] Total Scratch Memory: 0\n",
      "[06/27/2022-08:30:56] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 1 MiB\n",
      "[06/27/2022-08:30:56] [TRT] [V] Optimized block assignment.\n",
      "[06/27/2022-08:30:56] [TRT] [I] Total Activation Memory: 0\n",
      "[06/27/2022-08:30:56] [TRT] [V] Using cublasLt as a tactic source\n",
      "[06/27/2022-08:30:56] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 793, GPU 3271 (MiB)\n",
      "[06/27/2022-08:30:56] [TRT] [V] Using cuDNN as a tactic source\n",
      "[06/27/2022-08:30:56] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 793, GPU 3271 (MiB)\n",
      "[06/27/2022-08:30:56] [TRT] [V] Engine generation completed in 0.276513 seconds.\n",
      "[06/27/2022-08:30:56] [TRT] [V] Engine Layer Information:\n",
      "Layer(PluginV2): (Unnamed Layer* 0) [PluginV2Ext], Tactic: 0, input[Float(1,3,225,225)] -> output[Float(1,3,112,112)]\n",
      "[06/27/2022-08:30:56] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)\n",
      "[06/27/2022-08:30:56] [TRT] [I] Loaded engine size: 0 MiB\n",
      "[06/27/2022-08:30:56] [TRT] [V] Using cublasLt as a tactic source\n",
      "[06/27/2022-08:30:56] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 793, GPU 3271 (MiB)\n",
      "[06/27/2022-08:30:56] [TRT] [V] Using cuDNN as a tactic source\n",
      "[06/27/2022-08:30:56] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 793, GPU 3271 (MiB)\n",
      "[06/27/2022-08:30:56] [TRT] [V] Deserialization required 5795 microseconds.\n",
      "[06/27/2022-08:30:56] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)\n",
      "[06/27/2022-08:30:56] [TRT] [V] Using cublasLt as a tactic source\n",
      "[06/27/2022-08:30:56] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 793, GPU 3271 (MiB)\n",
      "[06/27/2022-08:30:56] [TRT] [V] Using cuDNN as a tactic source\n",
      "[06/27/2022-08:30:56] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 793, GPU 3271 (MiB)\n",
      "[06/27/2022-08:30:56] [TRT] [V] Total per-runner device persistent memory is 0\n",
      "[06/27/2022-08:30:56] [TRT] [V] Total per-runner host persistent memory is 16\n",
      "[06/27/2022-08:30:56] [TRT] [V] Allocated activation device memory of size 0\n",
      "[06/27/2022-08:30:56] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)\n",
      "Process started.\n",
      "Process finished.\n"
     ]
    }
   ],
   "source": [
    "net_dict = None\n",
    "with build_engine2(TRT_LOGGER) as engine:\n",
    "    inputs, outputs, bindings, stream = common.allocate_buffers(engine)\n",
    "    with engine.create_execution_context() as context:\n",
    "        inputs[0].host = input_batch.numpy()\n",
    "        trt_outputs = common.do_inference_v2( \\\n",
    "            context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Inputs</th>\n",
       "      <th>Outputs</th>\n",
       "      <th>Type Specific Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Unnamed Layer* 0) [PluginV2Ext]</td>\n",
       "      <td>LayerType.PLUGIN_V2</td>\n",
       "      <td>(1, 3, 225, 225)</td>\n",
       "      <td>(1, 3, 112, 112)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Name                 Type             Inputs  \\\n",
       "0  (Unnamed Layer* 0) [PluginV2Ext]  LayerType.PLUGIN_V2   (1, 3, 225, 225)   \n",
       "\n",
       "             Outputs Type Specific Params  \n",
       "0   (1, 3, 112, 112)                       "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(net_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.2468637  0.9731418  1.7283603  ... 1.7039526  1.7803113  1.3318315 ]\n",
      "  [0.87263244 1.6231679  1.7283603  ... 1.0964074  1.6265202  2.3499799 ]\n",
      "  [0.87263244 0.8002932  0.8002932  ... 1.4108361  1.6265202  2.3499799 ]\n",
      "  ...\n",
      "  [1.8190268  2.0853243  2.0853243  ... 2.7980096  2.7980096  1.5694978 ]\n",
      "  [0.85090864 2.0072181  2.0072181  ... 2.075235   1.7450305  0.03003781]\n",
      "  [0.99545527 2.0072181  2.0072181  ... 2.075235   1.7450305  0.09410294]]\n",
      "\n",
      " [[1.6933644  1.4559153  0.5049344  ... 1.1002792  1.566453   1.4299002 ]\n",
      "  [2.2559497  2.2559497  0.36667484 ... 1.1002792  1.566453   0.83345956]\n",
      "  [2.2559497  2.2559497  1.0379118  ... 2.6732693  1.2995731  1.2995731 ]\n",
      "  ...\n",
      "  [1.3822354  1.1411262  1.017575   ... 1.4276408  1.6172732  1.8445979 ]\n",
      "  [1.7493899  1.7493899  1.4588192  ... 1.4615191  1.6172732  1.0922401 ]\n",
      "  [1.9097644  1.7493899  1.6592412  ... 1.4615191  1.2949623  0.6548751 ]]\n",
      "\n",
      " [[1.7477155  1.7477155  1.4374763  ... 1.4319489  1.9334582  1.9334582 ]\n",
      "  [1.7477155  1.7477155  1.4770554  ... 1.823906   1.823906   0.9343786 ]\n",
      "  [1.4112515  2.1228704  2.1228704  ... 1.9349664  2.7011197  1.1517128 ]\n",
      "  ...\n",
      "  [2.4014568  2.9250915  2.9250915  ... 0.70698655 0.5312173  1.7811435 ]\n",
      "  [0.8799799  2.9250915  2.9250915  ... 0.92300457 2.0150304  2.0150304 ]\n",
      "  [1.9877678  1.4277685  1.18733    ... 1.6709127  2.0669277  1.0536616 ]]]\n"
     ]
    }
   ],
   "source": [
    "result = trt_outputs[0].reshape((3, output_size, output_size))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(sum(abs(result.flatten() - reference.flatten())) / len(result.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
